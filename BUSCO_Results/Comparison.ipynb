{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a4db210-b3a7-46b4-a41b-f162db040d85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        # Busco id_x      Status # Busco id_y\n",
      "0           15at3041    Complete     15at3041\n",
      "1           15at3041    Complete     42at3041\n",
      "2           15at3041    Complete     45at3041\n",
      "3           15at3041    Complete     52at3041\n",
      "4           15at3041    Complete     64at3041\n",
      "...              ...         ...          ...\n",
      "1909048  12103at3041  Fragmented   9687at3041\n",
      "1909049  12103at3041  Fragmented  10253at3041\n",
      "1909050  12103at3041  Fragmented  10960at3041\n",
      "1909051  12103at3041  Fragmented  11481at3041\n",
      "1909052  12103at3041  Fragmented  12131at3041\n",
      "\n",
      "[1909053 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df1 = pd.read_csv('Cleaned_Chlorella_A99.tsv', sep='\\t', skiprows=2)\n",
    "df1 = df1.iloc[:, :2]\n",
    "df2 = pd.read_csv('Cleaned_Micractinium_conductrix_SAG24180.tsv', sep='\\t', skiprows=2)\n",
    "df2 = df2.iloc[:, :2]\n",
    "df3 = pd.read_csv('Cleaned_Scenedesmus_PABB004.tsv', sep='\\t', skiprows=2)\n",
    "df3 = df3.iloc[:, :2]\n",
    "\n",
    "merged_df = pd.merge(df1, df2, on='Status', how='inner')\n",
    "#merged_df = pd.merge(merged_df, df3, on='Status', how='inner')\n",
    "\n",
    "print(merged_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ba97c249-0162-4b40-963f-7b2a3f707861",
   "metadata": {},
   "outputs": [
    {
     "ename": "ParserError",
     "evalue": "Error tokenizing data. C error: Expected 1 fields in line 3, saw 10\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mParserError\u001b[0m                               Traceback (most recent call last)",
      "Input \u001b[0;32mIn [7]\u001b[0m, in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m      3\u001b[0m df_list \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCleaned_Chlorella_A99.tsv\u001b[39m\u001b[38;5;124m'\u001b[39m, sep\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m'\u001b[39m, chunksize\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m500\u001b[39m):\n\u001b[1;32m      5\u001b[0m     df_list\u001b[38;5;241m.\u001b[39mappend(chunk)\n\u001b[1;32m      6\u001b[0m df1 \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat(df_list)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/pandas/io/parsers/readers.py:1187\u001b[0m, in \u001b[0;36mTextFileReader.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1185\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__next__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m   1186\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1187\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_chunk\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1188\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[1;32m   1189\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/pandas/io/parsers/readers.py:1284\u001b[0m, in \u001b[0;36mTextFileReader.get_chunk\u001b[0;34m(self, size)\u001b[0m\n\u001b[1;32m   1282\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m\n\u001b[1;32m   1283\u001b[0m     size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmin\u001b[39m(size, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnrows \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_currow)\n\u001b[0;32m-> 1284\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/pandas/io/parsers/readers.py:1254\u001b[0m, in \u001b[0;36mTextFileReader.read\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   1252\u001b[0m nrows \u001b[38;5;241m=\u001b[39m validate_integer(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnrows\u001b[39m\u001b[38;5;124m\"\u001b[39m, nrows)\n\u001b[1;32m   1253\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1254\u001b[0m     index, columns, col_dict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1255\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m   1256\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/pandas/io/parsers/c_parser_wrapper.py:225\u001b[0m, in \u001b[0;36mCParserWrapper.read\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m    223\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    224\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlow_memory:\n\u001b[0;32m--> 225\u001b[0m         chunks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_reader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_low_memory\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    226\u001b[0m         \u001b[38;5;66;03m# destructive to chunks\u001b[39;00m\n\u001b[1;32m    227\u001b[0m         data \u001b[38;5;241m=\u001b[39m _concatenate_chunks(chunks)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/pandas/_libs/parsers.pyx:817\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader.read_low_memory\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/pandas/_libs/parsers.pyx:861\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/pandas/_libs/parsers.pyx:847\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/pandas/_libs/parsers.pyx:1960\u001b[0m, in \u001b[0;36mpandas._libs.parsers.raise_parser_error\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mParserError\u001b[0m: Error tokenizing data. C error: Expected 1 fields in line 3, saw 10\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_list = []\n",
    "for chunk in pd.read_csv('Cleaned_Chlorella_A99.tsv', sep='\\t', chunksize=500):\n",
    "    df_list.append(chunk)\n",
    "df1 = pd.concat(df_list)\n",
    "\n",
    "df_list = []\n",
    "for chunk in pd.read_csv('Cleaned_Micractinium_conductrix_SAG24180.tsv', sep='\\t', chunksize=500):\n",
    "    df_list.append(chunk)\n",
    "df2 = pd.concat(df_list)\n",
    "\n",
    "df_list = []\n",
    "for chunk in pd.read_csv('Cleaned_Scenedesmus_PABB004.tsv', sep='\\t', chunksize=500):\n",
    "    df_list.append(chunk)\n",
    "df3 = pd.concat(df_list)\n",
    "\n",
    "merged_df = pd.merge(df1, df2, on='Status', how='inner')\n",
    "merged_df = pd.merge(merged_df, df3, on='Status', how='inner')\n",
    "\n",
    "common_columns = list(set(df1.columns) & set(df2.columns) & set(df3.columns))\n",
    "merged_df = merged_df[common_columns]\n",
    "\n",
    "print(merged_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
